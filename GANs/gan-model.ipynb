{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:41:33.302698Z","iopub.execute_input":"2025-01-28T12:41:33.302967Z","iopub.status.idle":"2025-01-28T12:41:45.247008Z","shell.execute_reply.started":"2025-01-28T12:41:33.302943Z","shell.execute_reply":"2025-01-28T12:41:45.246389Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load the dataset\n(train_images, _), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\ntrain_images = train_images / 255.0  # Normalize to [0, 1]\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:42:31.105066Z","iopub.execute_input":"2025-01-28T12:42:31.105377Z","iopub.status.idle":"2025-01-28T12:42:32.149747Z","shell.execute_reply.started":"2025-01-28T12:42:31.105354Z","shell.execute_reply":"2025-01-28T12:42:32.148979Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Hyperparameters\nBUFFER_SIZE = 60000\nBATCH_SIZE = 256\nEPOCHS = 50\nNOISE_DIM = 100\nNUM_IMAGES_TO_SAVE = 100\nOUTPUT_FOLDER = \"gan_generated_images\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:43:20.346998Z","iopub.execute_input":"2025-01-28T12:43:20.347275Z","iopub.status.idle":"2025-01-28T12:43:20.351214Z","shell.execute_reply.started":"2025-01-28T12:43:20.347256Z","shell.execute_reply":"2025-01-28T12:43:20.350444Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Prepare the dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:43:57.398874Z","iopub.execute_input":"2025-01-28T12:43:57.399192Z","iopub.status.idle":"2025-01-28T12:43:58.725803Z","shell.execute_reply.started":"2025-01-28T12:43:57.399170Z","shell.execute_reply":"2025-01-28T12:43:58.724789Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Define the Generator\ndef build_generator():\n    model = models.Sequential([\n        layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(NOISE_DIM,)),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n        layers.Reshape((7, 7, 256)),\n        layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(),\n        layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False, activation=\"tanh\"),\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:44:30.117025Z","iopub.execute_input":"2025-01-28T12:44:30.117303Z","iopub.status.idle":"2025-01-28T12:44:30.122736Z","shell.execute_reply.started":"2025-01-28T12:44:30.117284Z","shell.execute_reply":"2025-01-28T12:44:30.121918Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define the Discriminator\ndef build_discriminator():\n    model = models.Sequential([\n        layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\", input_shape=[28, 28, 1]),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n        layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(),\n        layers.Dropout(0.3),\n        layers.Flatten(),\n        layers.Dense(1),\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:45:26.776816Z","iopub.execute_input":"2025-01-28T12:45:26.777124Z","iopub.status.idle":"2025-01-28T12:45:26.781841Z","shell.execute_reply.started":"2025-01-28T12:45:26.777100Z","shell.execute_reply":"2025-01-28T12:45:26.780945Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Loss functions\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:46:25.974271Z","iopub.execute_input":"2025-01-28T12:46:25.974573Z","iopub.status.idle":"2025-01-28T12:46:25.978424Z","shell.execute_reply.started":"2025-01-28T12:46:25.974552Z","shell.execute_reply":"2025-01-28T12:46:25.977287Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    return real_loss + fake_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:46:33.256103Z","iopub.execute_input":"2025-01-28T12:46:33.256378Z","iopub.status.idle":"2025-01-28T12:46:33.260415Z","shell.execute_reply.started":"2025-01-28T12:46:33.256359Z","shell.execute_reply":"2025-01-28T12:46:33.259506Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:46:42.330594Z","iopub.execute_input":"2025-01-28T12:46:42.330876Z","iopub.status.idle":"2025-01-28T12:46:42.334842Z","shell.execute_reply.started":"2025-01-28T12:46:42.330855Z","shell.execute_reply":"2025-01-28T12:46:42.333810Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Optimizers\ngenerator = build_generator()\ndiscriminator = build_discriminator()\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:48:42.554558Z","iopub.execute_input":"2025-01-28T12:48:42.554875Z","iopub.status.idle":"2025-01-28T12:48:44.119384Z","shell.execute_reply.started":"2025-01-28T12:48:42.554851Z","shell.execute_reply":"2025-01-28T12:48:44.118448Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Training step\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:51:30.630089Z","iopub.execute_input":"2025-01-28T12:51:30.630386Z","iopub.status.idle":"2025-01-28T12:51:30.635979Z","shell.execute_reply.started":"2025-01-28T12:51:30.630366Z","shell.execute_reply":"2025-01-28T12:51:30.635218Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Generate and save images\ndef save_generated_images(epoch, generator, num_images=NUM_IMAGES_TO_SAVE):\n    noise = tf.random.normal([num_images, NOISE_DIM])\n    generated_images = generator(noise, training=False)\n\n    folder = os.path.join(OUTPUT_FOLDER, f\"epoch_{epoch:03d}\")\n    os.makedirs(folder, exist_ok=True)\n\n    for i in range(num_images):\n        image = generated_images[i, :, :, 0] * 127.5 + 127.5  # Rescale to [0, 255]\n        plt.imsave(os.path.join(folder, f\"image_{i+1:03d}.png\"), image, cmap=\"gray\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:51:47.605796Z","iopub.execute_input":"2025-01-28T12:51:47.606134Z","iopub.status.idle":"2025-01-28T12:51:47.611386Z","shell.execute_reply.started":"2025-01-28T12:51:47.606110Z","shell.execute_reply":"2025-01-28T12:51:47.610577Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Training loop\ndef train(dataset, epochs):\n    for epoch in range(1, epochs + 1):\n        for image_batch in dataset:\n            train_step(image_batch)\n\n        print(f\"Epoch {epoch}/{epochs} completed.\")\n        save_generated_images(epoch, generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:52:01.304158Z","iopub.execute_input":"2025-01-28T12:52:01.304442Z","iopub.status.idle":"2025-01-28T12:52:01.308685Z","shell.execute_reply.started":"2025-01-28T12:52:01.304421Z","shell.execute_reply":"2025-01-28T12:52:01.307955Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Train the GAN\ntrain(train_dataset, EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T12:52:49.507979Z","iopub.execute_input":"2025-01-28T12:52:49.508270Z","iopub.status.idle":"2025-01-28T13:03:24.593419Z","shell.execute_reply.started":"2025-01-28T12:52:49.508249Z","shell.execute_reply":"2025-01-28T13:03:24.592513Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50 completed.\nEpoch 2/50 completed.\nEpoch 3/50 completed.\nEpoch 4/50 completed.\nEpoch 5/50 completed.\nEpoch 6/50 completed.\nEpoch 7/50 completed.\nEpoch 8/50 completed.\nEpoch 9/50 completed.\nEpoch 10/50 completed.\nEpoch 11/50 completed.\nEpoch 12/50 completed.\nEpoch 13/50 completed.\nEpoch 14/50 completed.\nEpoch 15/50 completed.\nEpoch 16/50 completed.\nEpoch 17/50 completed.\nEpoch 18/50 completed.\nEpoch 19/50 completed.\nEpoch 20/50 completed.\nEpoch 21/50 completed.\nEpoch 22/50 completed.\nEpoch 23/50 completed.\nEpoch 24/50 completed.\nEpoch 25/50 completed.\nEpoch 26/50 completed.\nEpoch 27/50 completed.\nEpoch 28/50 completed.\nEpoch 29/50 completed.\nEpoch 30/50 completed.\nEpoch 31/50 completed.\nEpoch 32/50 completed.\nEpoch 33/50 completed.\nEpoch 34/50 completed.\nEpoch 35/50 completed.\nEpoch 36/50 completed.\nEpoch 37/50 completed.\nEpoch 38/50 completed.\nEpoch 39/50 completed.\nEpoch 40/50 completed.\nEpoch 41/50 completed.\nEpoch 42/50 completed.\nEpoch 43/50 completed.\nEpoch 44/50 completed.\nEpoch 45/50 completed.\nEpoch 46/50 completed.\nEpoch 47/50 completed.\nEpoch 48/50 completed.\nEpoch 49/50 completed.\nEpoch 50/50 completed.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}